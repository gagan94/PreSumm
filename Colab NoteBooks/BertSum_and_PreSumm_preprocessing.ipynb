{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertSum and PreSumm -preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITFHDGZalHiM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "outputId": "b2b1bdd9-362e-48b2-aeee-8e597fe00281"
      },
      "source": [
        "import pandas as pd\n",
        "!pip install stanfordnlp\n",
        "import stanfordnlp\n",
        "!pip install pytorch_pretrained_bert\n",
        "import torch,gc"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stanfordnlp in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (4.28.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (2.21.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.17.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2.8)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (42.0.2)\n",
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.17.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.10.47)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.13.47)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch_pretrained_bert) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.47->boto3->pytorch_pretrained_bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE30QzmGBeGC",
        "colab_type": "text"
      },
      "source": [
        "CoreNLP running on Python Server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2pQHQ_Vx4n8",
        "colab_type": "code",
        "outputId": "85201464-8d7e-4146-a759-24388cb1bcf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!echo \"Downloading CoreNLP...\"\n",
        "!wget \"http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\" -O corenlp.zip\n",
        "!unzip corenlp.zip\n",
        "!mv ./stanford-corenlp-full-2018-10-05 ./corenlp\n",
        "\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = \"./corenlp\"\n",
        "\n",
        "# Import client module\n",
        "from stanfordnlp.server import CoreNLPClient"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading CoreNLP...\n",
            "--2020-01-23 06:13:53--  http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip [following]\n",
            "--2020-01-23 06:13:54--  https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 393239982 (375M) [application/zip]\n",
            "Saving to: ‘corenlp.zip’\n",
            "\n",
            "corenlp.zip         100%[===================>] 375.02M  7.33MB/s    in 2m 2s   \n",
            "\n",
            "2020-01-23 06:15:56 (3.07 MB/s) - ‘corenlp.zip’ saved [393239982/393239982]\n",
            "\n",
            "Archive:  corenlp.zip\n",
            "   creating: stanford-corenlp-full-2018-10-05/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-core-2.3.0.1-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/xom-1.2.10-src.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/CoreNLP-to-HTML.xsl  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/README.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jollyday-0.4.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/LIBRARY-LICENSES  \n",
            "   creating: stanford-corenlp-full-2018-10-05/sutime/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/british.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/defs.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/spanish.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/english.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/english.holidays.sutime.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/ejml-0.23-src.zip  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/input.txt.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/build.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/pom.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-javadoc.jar  \n",
            "   creating: stanford-corenlp-full-2018-10-05/tokensregex/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.input.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/retokenize.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.properties  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.rules.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.json-api-1.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-api-2.4.0-b180830.0359-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-models.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/protobuf.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.activation-api-1.2.0.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/StanfordDependenciesManual.pdf  \n",
            "   creating: stanford-corenlp-full-2018-10-05/patterns/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/example.properties  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/otherpeople.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/goldplaces.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/stopwords.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/presidents.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/names.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/places.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/goldnames.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/slf4j-simple.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/input.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/joda-time.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/xom.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-impl-2.4.0-b180830.0438-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/StanfordCoreNlpDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-core-2.3.0.1.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/RESOURCE-LICENSES  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.activation-api-1.2.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/slf4j-api.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/pom-java-11.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/ejml-0.23.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.json.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/Makefile  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/corenlp.sh  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/joda-time-2.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-api-2.4.0-b180830.0359.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jollyday.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/ShiftReduceDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-impl-2.4.0-b180830.0438.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/SemgrexDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/LICENSE.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ_xnuObx-Tq",
        "colab_type": "code",
        "outputId": "3e3ad2d0-ea3f-41fa-e68c-980ca9f0cbe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\n",
        "client = CoreNLPClient(annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner'], memory='4G', endpoint='http://localhost:9001')\n",
        "print(client)\n",
        "\n",
        "# Start the background server and wait for some time\n",
        "# Note that in practice this is totally optional, as by default the server will be started when the first annotation is performed\n",
        "client.start()\n",
        "import time; time.sleep(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<stanfordnlp.server.client.CoreNLPClient object at 0x7f6ed0cf68d0>\n",
            "Starting server with command: java -Xmx4G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-5f4e4d7044944e52.props -preload tokenize,ssplit,pos,lemma,ner\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7dcco3twlqN",
        "colab_type": "text"
      },
      "source": [
        "Add the Number of Rows to be considered as training data, test data and validation data in the input CSV which contains the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGqQRV7dwq92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Trainrow = 1\n",
        "testrow = 45\n",
        "valrow = 49"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxuT_e34DLnb",
        "colab_type": "text"
      },
      "source": [
        "Reading the input text from CSV file mentioned here. Change according to requirement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn202rSM-MpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "def format_to_lines():\n",
        "    p_ct = 0\n",
        "    i = 0\n",
        "    for i in range(Trainrow):\n",
        "        df_csv = pd.read_csv('./fincident.csv',encoding='cp1252')\n",
        "        dataset = []\n",
        "        d = _format_to_lines_new(df_csv,i)\n",
        "        dataset.append(d)\n",
        "        i += 1\n",
        "        pt_file = \"{:s}.{:s}.{:d}.json\".format('bert_data', \"train\", p_ct)\n",
        "        with open(pt_file, 'w') as save:\n",
        "        # save.write('\\n'.join(dataset))\n",
        "          save.write(json.dumps(dataset))\n",
        "          p_ct += 1\n",
        "          print(p_ct)\n",
        "          dataset = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKBI7ZxU8Qzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "REMAP = {\"-lrb-\": \"(\", \"-rrb-\": \")\", \"-lcb-\": \"{\", \"-rcb-\": \"}\",\n",
        "         \"-lsb-\": \"[\", \"-rsb-\": \"]\", \"``\": '\"', \"''\": '\"'}\n",
        "\n",
        "import re\n",
        "def clean(x):\n",
        "    return re.sub(\n",
        "        r\"-lrb-|-rrb-|-lcb-|-rcb-|-lsb-|-rsb-|``|''\",\n",
        "        lambda m: REMAP.get(m.group()), x)\n",
        "    \n",
        "lower = True\n",
        "\n",
        "def _format_to_lines_new(df_csv,i):\n",
        "  source = []\n",
        "  tgt = []\n",
        "  flag = False\n",
        "  print(df_csv['Texts'][i])\n",
        "  ann = client.annotate(str(df_csv['Texts'][i]),annotators='tokenize,ssplit', output_format='json' )\n",
        "  for sent in ann['sentences']:\n",
        "    tokens = [t['word'] for t in sent['tokens']]\n",
        "    if (lower):\n",
        "        tokens = [t.lower() for t in tokens]\n",
        "    if (tokens[0] == '@highlight'):\n",
        "        flag = True\n",
        "        continue\n",
        "    if (flag):\n",
        "        tgt.append(tokens)\n",
        "        flag = False\n",
        "    else:\n",
        "        source.append(tokens)\n",
        "\n",
        "\n",
        "  ann = client.annotate(str(df_csv['Analysis'][i]),annotators='tokenize,ssplit', output_format='json' )\n",
        "  for sent in ann['sentences']:\n",
        "    tokens = [t['word'] for t in sent['tokens']]\n",
        "    if (lower):\n",
        "        tokens = [t.lower() for t in tokens]\n",
        "    if (tokens[0] == '@highlight'):\n",
        "        flag = True\n",
        "        continue\n",
        "    if (flag):\n",
        "        tgt.append(tokens)\n",
        "        flag = False\n",
        "    else:\n",
        "        tgt.append(tokens)\n",
        "    \n",
        "\n",
        "  source = [clean(' '.join(sent)).split() for sent in source]\n",
        "  tgt = [clean(' '.join(sent)).split() for sent in tgt]\n",
        "\n",
        "  return {'src': source, 'tgt': tgt}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM29IMF3JytO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from multiprocess import Pool\n",
        "def format_to_bert(args):\n",
        "    for i in range(Trainrow):\n",
        "        a_lst = []\n",
        "        for json_f in glob.glob(pjoin(args.raw_path, '*' + corpus_type + '.*.json')):\n",
        "            real_name = json_f.split('/')[-1]\n",
        "            a_lst.append((json_f, args, pjoin(args.save_path, real_name.replace('json', 'bert.pt'))))\n",
        "        print(a_lst)\n",
        "        pool = Pool(args.n_cpus)\n",
        "        for d in pool.imap(_format_to_bert, a_lst):\n",
        "            pass\n",
        "\n",
        "        pool.close()\n",
        "        pool.join()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WoQ2yY6Dc30",
        "colab_type": "text"
      },
      "source": [
        "Genereate JSON file of the trianing data from th CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY9D4ibMw3Wy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "9a2d0428-e368-4ea9-f98e-1616a192f4a2"
      },
      "source": [
        "format_to_lines()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Hello Colleagues The solution creation is failing in QTC 906 with all the users with generic error message saying authorisation is missing. but with the same users the solutions were created previously also. NOTE : Even after the error message is shown the solution can be seen in solution explorer window but when clicked on it to sync then it says solution does not exist in the system. can you please check this ? This is blocking us for creating solution to test our automates. Regards Vishwanath A. Telsang \n",
            "\n",
            " Hi Colleagues The issue is coming up because .myproj is not getting created in XREP. Please take over and check what can be done. Thanks  Regards Sasi. \n",
            "\n",
            " Hi I debugged the SDk and found out thar we call the PDI_1o_product_create to create product and after the product creation is done we make a separate call to generate solution file  (.myproj .sln and .suo  ) The load in system QTC 906 is too heavy. thera are in total 4700 solution present in the tenant. Since the system load is too high the solution creation is being timedout and we can make a backend call for creating these solution realted files. To resolve the issue we need to make system faster by removing the unwanted solution. To workaround: 1. Create a dummy solution in any tenant 2. Do a download as copy and deploy to QTC 906. 3. Create patch in 906 . 4. Use this solution for automate related task. Regards Nadeem \n",
            "\n",
            " Hi I debugged the SDk and found out thar we call the PDI_1o_product_create to create product and after the product creation is done we make a separate call to generate solution file (.myproj .sln and .suo ) The load in system QTC 906 is too heavy. thera are in total 4700 solution present in the tenant. Since the system load is too high the solution creation is being timedout and we can make a backend call for creating these solution realted files. To resolve the issue we need to make system faster by removing the unwanted solution. To workaround: 1. Create a dummy solution in any tenant 2. Do a download as copy and deploy to QTC 906. 3. Create patch in 906 . 4. Use this solution for automate related task. Regards Nadeem \n",
            "\n",
            "\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySCVJ3m4DpvF",
        "colab_type": "text"
      },
      "source": [
        "Different Modules which are required below for Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd9SbZoMP17y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_word_ngrams(n, sentences):\n",
        "    \"\"\"Calculates word n-grams for multiple sentences.\n",
        "    \"\"\"\n",
        "    assert len(sentences) > 0\n",
        "    assert n > 0\n",
        "\n",
        "    # words = _split_into_words(sentences)\n",
        "\n",
        "    words = sum(sentences, [])\n",
        "    # words = [w for w in words if w not in stopwords]\n",
        "    return _get_ngrams(n, words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnxFEX8zP8VM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_ngrams(n, text):\n",
        "    \"\"\"Calcualtes n-grams.\n",
        "    Args:\n",
        "      n: which n-grams to calculate\n",
        "      text: An array of tokens\n",
        "    Returns:\n",
        "      A set of n-grams\n",
        "    \"\"\"\n",
        "    ngram_set = set()\n",
        "    text_length = len(text)\n",
        "    max_index_ngram_start = text_length - n\n",
        "    for i in range(max_index_ngram_start + 1):\n",
        "        ngram_set.add(tuple(text[i:i + n]))\n",
        "    return ngram_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzIYN1pDPnQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
        "    def _rouge_clean(s):\n",
        "        return re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
        "\n",
        "    max_rouge = 0.0\n",
        "    abstract = sum(abstract_sent_list, [])\n",
        "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
        "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
        "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
        "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
        "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
        "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
        "\n",
        "    selected = []\n",
        "    for s in range(summary_size):\n",
        "        cur_max_rouge = max_rouge\n",
        "        cur_id = -1\n",
        "        for i in range(len(sents)):\n",
        "            if (i in selected):\n",
        "                continue\n",
        "            c = selected + [i]\n",
        "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
        "            candidates_1 = set.union(*map(set, candidates_1))\n",
        "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
        "            candidates_2 = set.union(*map(set, candidates_2))\n",
        "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
        "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
        "            rouge_score = rouge_1 + rouge_2\n",
        "            if rouge_score > cur_max_rouge:\n",
        "                cur_max_rouge = rouge_score\n",
        "                cur_id = i\n",
        "        if (cur_id == -1):\n",
        "            return selected\n",
        "        selected.append(cur_id)\n",
        "        max_rouge = cur_max_rouge\n",
        "\n",
        "    return sorted(selected)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIOJn85cQFKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_rouge(evaluated_ngrams, reference_ngrams):\n",
        "    reference_count = len(reference_ngrams)\n",
        "    evaluated_count = len(evaluated_ngrams)\n",
        "\n",
        "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
        "    overlapping_count = len(overlapping_ngrams)\n",
        "\n",
        "    if evaluated_count == 0:\n",
        "        precision = 0.0\n",
        "    else:\n",
        "        precision = overlapping_count / evaluated_count\n",
        "\n",
        "    if reference_count == 0:\n",
        "        recall = 0.0\n",
        "    else:\n",
        "        recall = overlapping_count / reference_count\n",
        "\n",
        "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
        "    return {\"f\": f1_score, \"p\": precision, \"r\": recall}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw0Sl56pPbJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_pretrained_bert import BertTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWzzl_OCE1yh",
        "colab_type": "text"
      },
      "source": [
        "Execute the below code for Preprocessing for PreSumm Only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_DZhWoEaXDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For PreSumm\n",
        "\n",
        "class BertData():\n",
        "    def __init__(self):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "        self.sep_token = '[SEP]'\n",
        "        self.cls_token = '[CLS]'\n",
        "        self.pad_token = '[PAD]'\n",
        "        self.tgt_bos = '[unused0]'\n",
        "        self.tgt_eos = '[unused1]'\n",
        "        self.tgt_sent_split = '[unused2]'\n",
        "        self.sep_vid = self.tokenizer.vocab[self.sep_token]\n",
        "        self.cls_vid = self.tokenizer.vocab[self.cls_token]\n",
        "        self.pad_vid = self.tokenizer.vocab[self.pad_token]\n",
        "\n",
        "    def preprocess(self, src, tgt, sent_labels, use_bert_basic_tokenizer=False, is_test=False):\n",
        "\n",
        "        if ((not is_test) and len(src) == 0):\n",
        "            print(\"returned none\")\n",
        "            return None\n",
        "\n",
        "        original_src_txt = [' '.join(s) for s in src]\n",
        "\n",
        "        idxs = [i for i, s in enumerate(src) if (len(s) > 5)]\n",
        "\n",
        "        _sent_labels = [0] * len(src)\n",
        "        for l in sent_labels:\n",
        "            _sent_labels[l] = 1\n",
        "\n",
        "        src = [src[i][:200] for i in idxs]\n",
        "        sent_labels = [_sent_labels[i] for i in idxs]\n",
        "        src = src[:200]\n",
        "        sent_labels = sent_labels[:200]\n",
        "\n",
        "        if ((not is_test) and len(src) < 3):\n",
        "            print(\"returned none\")\n",
        "            return None\n",
        "\n",
        "        src_txt = [' '.join(sent) for sent in src]\n",
        "        text = ' {} {} '.format(self.sep_token, self.cls_token).join(src_txt)\n",
        "\n",
        "        src_subtokens = self.tokenizer.tokenize(text)\n",
        "\n",
        "        src_subtokens = [self.cls_token] + src_subtokens + [self.sep_token]\n",
        "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
        "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
        "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
        "        segments_ids = []\n",
        "        for i, s in enumerate(segs):\n",
        "            if (i % 2 == 0):\n",
        "                segments_ids += s * [0]\n",
        "            else:\n",
        "                segments_ids += s * [1]\n",
        "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
        "        sent_labels = sent_labels[:len(cls_ids)]\n",
        "\n",
        "        tgt_subtokens_str = '[unused0] ' + ' [unused2] '.join(\n",
        "            [' '.join(self.tokenizer.tokenize(' '.join(tt))) for tt in tgt]) + ' [unused1]'\n",
        "        tgt_subtoken = tgt_subtokens_str.split()[:200]\n",
        "        if ((not is_test) and len(tgt_subtoken) < 20):\n",
        "            print(\"returned none <20\")\n",
        "            print(len(tgt_subtoken))\n",
        "            return None\n",
        "\n",
        "        tgt_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(tgt_subtoken)\n",
        "\n",
        "        tgt_txt = '<q>'.join([' '.join(tt) for tt in tgt])\n",
        "        src_txt = [original_src_txt[i] for i in idxs]\n",
        "\n",
        "        return src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9xGn4a-E5Yi",
        "colab_type": "text"
      },
      "source": [
        "Execute the below code for Preprocessing for BertSumm Only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "056VQK9NOruw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For BertSum\n",
        "class BertData():\n",
        "    def __init__(self):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "        self.sep_vid = self.tokenizer.vocab['[SEP]']\n",
        "        self.cls_vid = self.tokenizer.vocab['[CLS]']\n",
        "        self.pad_vid = self.tokenizer.vocab['[PAD]']\n",
        "\n",
        "    def preprocess(self, src, tgt, oracle_ids):\n",
        "\n",
        "        if (len(src) == 0):\n",
        "            return None\n",
        "\n",
        "        original_src_txt = [' '.join(s) for s in src]\n",
        "\n",
        "        labels = [0] * len(src)\n",
        "        for l in oracle_ids:\n",
        "            labels[l] = 1\n",
        "\n",
        "        idxs = [i for i, s in enumerate(src) if (len(s) > 5)]\n",
        "\n",
        "        src = [src[i][:200] for i in idxs]\n",
        "        labels = [labels[i] for i in idxs]\n",
        "        src = src[:200]\n",
        "        labels = labels[:200]\n",
        "\n",
        "        print(\"length of label\")\n",
        "        print(len(labels))\n",
        "        if (len(src) < 10):\n",
        "            return None\n",
        "        if (len(labels) == 0):\n",
        "            return None\n",
        "\n",
        "        src_txt = [' '.join(sent) for sent in src]\n",
        "        # text = [' '.join(ex['src_txt'][i].split()[:self.args.max_src_ntokens]) for i in idxs]\n",
        "        # text = [_clean(t) for t in text]\n",
        "        text = ' [SEP] [CLS] '.join(src_txt)\n",
        "        src_subtokens = self.tokenizer.tokenize(text)\n",
        "        src_subtokens = src_subtokens[:510]\n",
        "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
        "\n",
        "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
        "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
        "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
        "        segments_ids = []\n",
        "        for i, s in enumerate(segs):\n",
        "            if (i % 2 == 0):\n",
        "                segments_ids += s * [0]\n",
        "            else:\n",
        "                segments_ids += s * [1]\n",
        "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
        "        labels = labels[:len(cls_ids)]\n",
        "\n",
        "        tgt_txt = '<q>'.join([' '.join(tt) for tt in tgt])\n",
        "        src_txt = [original_src_txt[i] for i in idxs]\n",
        "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKHpehAsEtX0",
        "colab_type": "text"
      },
      "source": [
        "Execute the below code for Preprocessing for PreSumm Only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_am-jsJYMm9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## For PreSumm Only\n",
        "\n",
        "def _format_to_bert():\n",
        "  p_ct = 0\n",
        "  for i in range(Trainrow):\n",
        "    pt_file = \"{:s}.{:s}.{:d}.json\".format('bert_data', \"train\", p_ct)\n",
        "    json_file = pt_file\n",
        "    save_file = pt_file.replace('.json','.pt')\n",
        "    p_ct += 1\n",
        "    bert = BertData()\n",
        "\n",
        "    jobs = json.load(open(json_file))\n",
        "    datasets = []\n",
        "    for d in jobs:\n",
        "        source, tgt = d['src'], d['tgt']\n",
        "        print(\"inside\")\n",
        "        oracle_ids = greedy_selection(source, tgt, 3)\n",
        "        #elif (args.oracle_mode == 'combination'):\n",
        "         #   oracle_ids = combination_selection(source, tgt, 3)\n",
        "        print(oracle_ids)\n",
        "        b_data = bert.preprocess(source, tgt, oracle_ids)\n",
        "        if (b_data is None):\n",
        "            print(\"None\")\n",
        "            continue\n",
        "        src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
        "        #for BertSumm\n",
        "        #indexed_tokens, labels, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
        "        # for BertSumm\n",
        "        #b_data_dict = {\"src\": indexed_tokens, \"labels\": labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
        "        #               'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n",
        "        b_data_dict = {\"src\": src_subtoken_idxs, \"tgt\": tgt_subtoken_idxs,\n",
        "                       \"src_sent_labels\": sent_labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
        "                       'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n",
        "        print(b_data_dict)\n",
        "        datasets.append(b_data_dict)\n",
        "    torch.save(datasets, save_file)\n",
        "    datasets = []\n",
        "    gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTHKvuh2EypV",
        "colab_type": "text"
      },
      "source": [
        "Execute the below code for Preprocessing for PreSumm Only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GAvF7YdESPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## For BertSumm Only\n",
        "\n",
        "def _format_to_bert():\n",
        "  p_ct = 0\n",
        "  for i in range(Trainrow):\n",
        "    pt_file = \"{:s}.{:s}.{:d}.json\".format('bert_data', \"train\", p_ct)\n",
        "    json_file = pt_file\n",
        "    save_file = pt_file.replace('.json','.pt')\n",
        "    p_ct += 1\n",
        "    bert = BertData()\n",
        "\n",
        "    jobs = json.load(open(json_file))\n",
        "    datasets = []\n",
        "    for d in jobs:\n",
        "        source, tgt = d['src'], d['tgt']\n",
        "        print(\"inside\")\n",
        "        oracle_ids = greedy_selection(source, tgt, 3)\n",
        "        #elif (args.oracle_mode == 'combination'):\n",
        "         #   oracle_ids = combination_selection(source, tgt, 3)\n",
        "        print(oracle_ids)\n",
        "        b_data = bert.preprocess(source, tgt, oracle_ids)\n",
        "        if (b_data is None):\n",
        "            print(\"None\")\n",
        "            continue\n",
        "        #for BertSumm\n",
        "        indexed_tokens, labels, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
        "        # for BertSumm\n",
        "        b_data_dict = {\"src\": indexed_tokens, \"labels\": labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
        "                       'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n",
        "        print(b_data_dict)\n",
        "        datasets.append(b_data_dict)\n",
        "    torch.save(datasets, save_file)\n",
        "    datasets = []\n",
        "    gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW7Ugu37EK8H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "90d2b3f2-5212-40dc-cd41-7a211d680744"
      },
      "source": [
        "_format_to_bert()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inside\n",
            "[0, 18, 22]\n",
            "{'src': [101, 7592, 8628, 1996, 5576, 4325, 2003, 7989, 1999, 1053, 13535, 3938, 2575, 2007, 2035, 1996, 5198, 2007, 12391, 7561, 4471, 3038, 3166, 6648, 2003, 4394, 1012, 102, 101, 2021, 2007, 1996, 2168, 5198, 1996, 7300, 2020, 2580, 3130, 2036, 1012, 102, 101, 3602, 1024, 2130, 2044, 1996, 7561, 4471, 2003, 3491, 1996, 5576, 2064, 2022, 2464, 1999, 5576, 10566, 3332, 2021, 2043, 13886, 2006, 2009, 2000, 26351, 2059, 2009, 2758, 5576, 2515, 2025, 4839, 1999, 1996, 2291, 1012, 102, 101, 2064, 2017, 3531, 4638, 2023, 1029, 102, 101, 2023, 2003, 10851, 2149, 2005, 4526, 5576, 2000, 3231, 2256, 8285, 15416, 1012, 102, 101, 12362, 25292, 18663, 16207, 1037, 1012, 10093, 8791, 2290, 7632, 8628, 1996, 3277, 2003, 2746, 2039, 2138, 1012, 102, 101, 2026, 21572, 3501, 2003, 2025, 2893, 2580, 1999, 1060, 2890, 2361, 1012, 102, 101, 3531, 2202, 2058, 1998, 4638, 2054, 2064, 2022, 2589, 1012, 102, 101, 7632, 1045, 2139, 8569, 15567, 1996, 17371, 2243, 1998, 2179, 2041, 22794, 2099, 2057, 2655, 1996, 22851, 2072, 1035, 1015, 2080, 1035, 4031, 1035, 3443, 2000, 3443, 4031, 1998, 2044, 1996, 4031, 4325, 2003, 2589, 2057, 2191, 1037, 3584, 2655, 2000, 9699, 5576, 5371, 1006, 1012, 102, 101, 10514, 2080, 1007, 1996, 7170, 1999, 2291, 1053, 13535, 3938, 2575, 2003, 2205, 3082, 1012, 102, 101, 1996, 2527, 2024, 1999, 2561, 21064, 2692, 5576, 2556, 1999, 1996, 16713, 1012, 102, 101, 2144, 1996, 2291, 7170, 2003, 2205, 2152, 1996, 5576, 4325, 2003, 2108, 22313, 5833, 1998, 2057, 2064, 2191, 1037, 2067, 10497, 2655, 2005, 4526, 2122, 5576, 2613, 3064, 6764, 1012, 102, 101, 2000, 10663, 1996, 3277, 2057, 2342, 2000, 2191, 2291, 5514, 2011, 9268, 1996, 18162, 5576, 1012, 102, 101, 3443, 1037, 24369, 5576, 1999, 2151, 16713, 1016, 1012, 102, 101, 2079, 1037, 8816, 2004, 6100, 1998, 21296, 2000, 1053, 13535, 3938, 2575, 1012, 102, 101, 2224, 2023, 5576, 2005, 8285, 8585, 3141, 4708, 1012, 102, 101, 12362, 23233, 21564, 7632, 1045, 2139, 8569, 15567, 1996, 17371, 2243, 1998, 2179, 2041, 22794, 2099, 2057, 2655, 1996, 22851, 2072, 1035, 1015, 2080, 1035, 4031, 1035, 3443, 2000, 3443, 4031, 1998, 2044, 1996, 4031, 4325, 2003, 2589, 2057, 2191, 1037, 3584, 2655, 2000, 9699, 5576, 5371, 1006, 1012, 102, 101, 10514, 2080, 1007, 1996, 7170, 1999, 2291, 1053, 13535, 3938, 2575, 2003, 2205, 3082, 1012, 102, 101, 1996, 2527, 2024, 1999, 2561, 21064, 2692, 5576, 2556, 1999, 1996, 16713, 1012, 102, 101, 2144, 1996, 2291, 7170, 2003, 2205, 2152, 1996, 5576, 4325, 2003, 2108, 22313, 5833, 1998, 2057, 2064, 2191, 1037, 2067, 10497, 2655, 2005, 4526, 2122, 5576, 2613, 3064, 6764, 1012, 102, 101, 2000, 10663, 1996, 3277, 2057, 2342, 2000, 2191, 2291, 5514, 2011, 9268, 1996, 18162, 5576, 1012, 102, 101, 3443, 1037, 24369, 5576, 1999, 2151, 16713, 1016, 1012, 102, 101, 2079, 1037, 8816, 2004, 6100, 1998, 21296, 2000, 1053, 13535, 3938, 2575, 1012, 102, 101, 2224, 2023, 5576, 2005, 8285, 8585, 3141, 4708, 1012, 102], 'tgt': [1, 1996, 5576, 4325, 2003, 7989, 1999, 1053, 13535, 3938, 2575, 2007, 2035, 1996, 5198, 2007, 12391, 7561, 4471, 3038, 3166, 6648, 2003, 4394, 1012, 3, 1996, 3277, 2003, 2746, 2039, 2138, 1012, 3, 2026, 21572, 3501, 2003, 2025, 2893, 2580, 1999, 1060, 2890, 2361, 1012, 3, 1996, 7170, 1999, 2291, 1053, 13535, 3938, 2575, 2003, 2205, 3082, 1012, 3, 2045, 2024, 1999, 2561, 21064, 2692, 5576, 2556, 1999, 1996, 16713, 1012, 3, 2144, 1996, 2291, 7170, 2003, 2205, 2152, 1996, 5576, 4325, 2003, 2108, 22313, 5833, 1012, 3, 2000, 2147, 24490, 1024, 1015, 1012, 3, 3443, 1037, 24369, 5576, 1999, 2151, 16713, 1016, 1012, 3, 2079, 1037, 8816, 2004, 6100, 1998, 21296, 2000, 1053, 13535, 3938, 2575, 1012, 3, 1017, 1012, 3, 3443, 8983, 1999, 3938, 2575, 1012, 3, 1018, 1012, 3, 2224, 2023, 5576, 2005, 8285, 8585, 3141, 4708, 1012, 2], 'src_sent_labels': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'segs': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'clss': [0, 28, 42, 80, 88, 103, 123, 137, 149, 197, 214, 229, 261, 279, 290, 305, 316, 367, 384, 399, 431, 449, 460, 475], 'src_txt': ['hello colleagues the solution creation is failing in qtc 906 with all the users with generic error message saying authorisation is missing .', 'but with the same users the solutions were created previously also .', 'note : even after the error message is shown the solution can be seen in solution explorer window but when clicked on it to sync then it says solution does not exist in the system .', 'can you please check this ?', 'this is blocking us for creating solution to test our automates .', 'regards vishwanath a. telsang hi colleagues the issue is coming up because .', 'myproj is not getting created in xrep .', 'please take over and check what can be done .', 'hi i debugged the sdk and found out thar we call the pdi _ 1o _ product _ create to create product and after the product creation is done we make a separate call to generate solution file ( .', 'suo ) the load in system qtc 906 is too heavy .', 'thera are in total 4700 solution present in the tenant .', 'since the system load is too high the solution creation is being timedout and we can make a backend call for creating these solution realted files .', 'to resolve the issue we need to make system faster by removing the unwanted solution .', 'create a dummy solution in any tenant 2 .', 'do a download as copy and deploy to qtc 906 .', 'use this solution for automate related task .', 'regards nadeem hi i debugged the sdk and found out thar we call the pdi _ 1o _ product _ create to create product and after the product creation is done we make a separate call to generate solution file ( .', 'suo ) the load in system qtc 906 is too heavy .', 'thera are in total 4700 solution present in the tenant .', 'since the system load is too high the solution creation is being timedout and we can make a backend call for creating these solution realted files .', 'to resolve the issue we need to make system faster by removing the unwanted solution .', 'create a dummy solution in any tenant 2 .', 'do a download as copy and deploy to qtc 906 .', 'use this solution for automate related task .'], 'tgt_txt': 'the solution creation is failing in qtc 906 with all the users with generic error message saying authorisation is missing .<q>the issue is coming up because .<q>myproj is not getting created in xrep .<q>the load in system qtc 906 is too heavy .<q>there are in total 4700 solution present in the tenant .<q>since the system load is too high the solution creation is being timedout .<q>to workaround : 1 .<q>create a dummy solution in any tenant 2 .<q>do a download as copy and deploy to qtc 906 .<q>3 .<q>create patch in 906 .<q>4 .<q>use this solution for automate related task .'}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}